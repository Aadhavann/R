{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10540384,"sourceType":"datasetVersion","datasetId":6521795}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# library(wordnet)\nlibrary(text2vec)\nlibrary(stringr)\nlibrary(topicmodels)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(kernlab)\nlibrary(rpart)\nlibrary(gbm)\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(MLmetrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:02.818129Z","iopub.execute_input":"2025-01-21T17:24:02.819955Z","iopub.status.idle":"2025-01-21T17:24:02.853391Z","shell.execute_reply":"2025-01-21T17:24:02.851406Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"#' Label Mapping Function\n\ncreate_label_mapping <- function() {\n  # Create mapping from original labels to valid R variable names\n  label_map <- c(\n    \"0\" = \"correct\",\n    \"1\" = \"contradictory\",\n    \"2\" = \"partially_correct\",\n    \"3\" = \"irrelevant\",\n    \"4\" = \"non_domain\"\n  )\n  return(label_map)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:04.029740Z","iopub.execute_input":"2025-01-21T17:24:04.031302Z","iopub.status.idle":"2025-01-21T17:24:04.045586Z","shell.execute_reply":"2025-01-21T17:24:04.043853Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"#' Reverse Label Mapping Function\n\nreverse_label_mapping <- function() {\n  # Create reverse mapping from valid R variable names to original labels\n  label_map <- c(\n    \"correct\" = 0,\n    \"contradictory\" = 1,\n    \"partially_correct\" = 2,\n    \"irrelevant\" = 3,\n    \"non_domain\" = 4\n  )\n  return(label_map)\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:05.859256Z","iopub.execute_input":"2025-01-21T17:24:05.860827Z","iopub.status.idle":"2025-01-21T17:24:05.875088Z","shell.execute_reply":"2025-01-21T17:24:05.873364Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Feature Engineering Functions","metadata":{}},{"cell_type":"markdown","source":"## 1. Semantic Similarity Features","metadata":{}},{"cell_type":"code","source":"initDict()\n\n\ncompute_wordnet_similarity <- function(text1, text2, measure) {\n  # Tokenize texts\n  words1 <- unlist(strsplit(tolower(text1), \"\\\\W+\"))\n  words2 <- unlist(strsplit(tolower(text2), \"\\\\W+\"))\n  \n  # Get synsets for each word\n  synsets1 <- lapply(words1, function(w) getSynsets(w, \"NOUN\"))\n  synsets2 <- lapply(words2, function(w) getSynsets(w, \"NOUN\"))\n  \n  # Compute similarities based on measure\n  similarities <- matrix(NA, length(synsets1), length(synsets2))\n  for(i in seq_along(synsets1)) {\n    for(j in seq_along(synsets2)) {\n      if(length(synsets1[[i]]) > 0 && length(synsets2[[j]]) > 0) {\n        sim <- switch(measure,\n                     \"path\" = getPathSimilarity(synsets1[[i]][[1]], synsets2[[j]][[1]]),\n                     \"lch\" = getLCHSimilarity(synsets1[[i]][[1]], synsets2[[j]][[1]]),\n                     \"wup\" = getWUPSimilarity(synsets1[[i]][[1]], synsets2[[j]][[1]]),\n                     # Add other WordNet measures here\n                     0)\n        similarities[i,j] <- sim\n      }\n    }\n  }\n  \n  # Return average similarity\n  mean(similarities, na.rm = TRUE)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:13.232195Z","iopub.execute_input":"2025-01-21T17:24:13.233868Z","iopub.status.idle":"2025-01-21T17:24:13.246815Z","shell.execute_reply":"2025-01-21T17:24:13.244961Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## 2. Lexical Overlap Features","metadata":{}},{"cell_type":"code","source":"compute_lexical_overlap <- function(text1, text2) {\n  # Tokenize texts\n  words1 <- unlist(strsplit(tolower(text1), \"\\\\W+\"))\n  words2 <- unlist(strsplit(tolower(text2), \"\\\\W+\"))\n  \n  # Jaccard similarity\n  intersection <- length(intersect(words1, words2))\n  union <- length(unique(c(words1, words2)))\n  jaccard <- if(union > 0) intersection / union else 0\n  \n  # Simple word overlap\n  simple_overlap <- if(min(length(words1), length(words2)) > 0) \n    intersection / min(length(words1), length(words2)) else 0\n  \n  # Return as named vector\n  return(c(jaccard = jaccard, simple_overlap = simple_overlap))\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:09.852759Z","iopub.execute_input":"2025-01-21T17:24:09.854555Z","iopub.status.idle":"2025-01-21T17:24:09.869047Z","shell.execute_reply":"2025-01-21T17:24:09.867311Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## 3. TF-IDF Features","metadata":{}},{"cell_type":"code","source":"compute_tfidf <- function(corpus) {\n  # Create vocabulary\n  it <- itoken(corpus, preprocessor = tolower, \n              tokenizer = word_tokenizer)\n  vocab <- create_vocabulary(it)\n  \n  # Prune vocabulary\n  vocab <- prune_vocabulary(vocab, term_count_min = 3)\n  \n  # Create document-term matrix\n  vectorizer <- vocab_vectorizer(vocab)\n  dtm <- create_dtm(it, vectorizer)\n  \n  # Compute TF-IDF\n  tfidf <- TfIdf$new()\n  dtm_tfidf <- fit_transform(dtm, tfidf)\n  \n  # Convert to dense matrix and then to data frame\n  dense_matrix <- as.matrix(dtm_tfidf)\n  \n  # Limit number of features to prevent memory issues\n  if(ncol(dense_matrix) > 100) {\n    dense_matrix <- dense_matrix[, 1:100]\n  }\n  \n  # Name columns\n  colnames(dense_matrix) <- paste0(\"tfidf_\", 1:ncol(dense_matrix))\n  \n  return(as.data.frame(dense_matrix))\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:15.391482Z","iopub.execute_input":"2025-01-21T17:24:15.393437Z","iopub.status.idle":"2025-01-21T17:24:15.408738Z","shell.execute_reply":"2025-01-21T17:24:15.406778Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## 4. Topic Modeling Features","metadata":{}},{"cell_type":"code","source":"compute_lda_features <- function(dtm, k = 10) {\n  # Fit LDA model\n  lda_model <- LDA(dtm, k = k)\n  \n  # Get document-topic distributions\n  doc_topics <- posterior(lda_model)$topics\n  \n  return(doc_topics)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:17.431437Z","iopub.execute_input":"2025-01-21T17:24:17.434471Z","iopub.status.idle":"2025-01-21T17:24:17.455286Z","shell.execute_reply":"2025-01-21T17:24:17.452309Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## 5. LSA Features","metadata":{}},{"cell_type":"code","source":"compute_lsa_features <- function(dtm, dims = 100) {\n  # Perform SVD\n  svd_model <- irlba::irlba(dtm, nv = dims)\n  \n  # Get document embeddings\n  doc_embeddings <- dtm %*% svd_model$v\n  \n  return(doc_embeddings)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:19.263380Z","iopub.execute_input":"2025-01-21T17:24:19.264936Z","iopub.status.idle":"2025-01-21T17:24:19.278812Z","shell.execute_reply":"2025-01-21T17:24:19.276835Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## 6. Top Scorer Features","metadata":{}},{"cell_type":"code","source":"compute_top_scorer_features <- function(student_answer, reference_answer, \n                                      top_answers, p = 5) {\n  # Combine reference answer with top P answers\n  enhanced_reference <- paste(c(reference_answer, \n                              head(top_answers, p)), \n                            collapse = \" \")\n  \n  # Compute LSA similarity with enhanced reference\n  lsa_sim <- cosine(as.matrix(student_answer), \n                    as.matrix(enhanced_reference))\n  \n  return(lsa_sim)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:20.806176Z","iopub.execute_input":"2025-01-21T17:24:20.807806Z","iopub.status.idle":"2025-01-21T17:24:20.821001Z","shell.execute_reply":"2025-01-21T17:24:20.819259Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Main Feature Extraction Function","metadata":{}},{"cell_type":"code","source":"extract_features <- function(data) {\n  # Initialize matrix for lexical features\n  n_samples <- nrow(data)\n  lex_features <- matrix(0, nrow = n_samples, ncol = 2)\n  colnames(lex_features) <- c(\"jaccard\", \"simple_overlap\")\n  \n  # Compute lexical overlap features\n  for(i in 1:n_samples) {\n    lex_features[i,] <- compute_lexical_overlap(\n      data$reference_answer[i], \n      data$student_answer[i]\n    )\n  }\n  \n  # Convert lexical features to data frame\n  features_df <- as.data.frame(lex_features)\n  \n  # Compute and add TF-IDF features\n  tfidf_features <- compute_tfidf(data$student_answer)\n  \n  # Combine features\n  features_df <- cbind(features_df, tfidf_features)\n  \n  # Replace any NAs with 0\n  features_df[is.na(features_df)] <- 0\n  \n  return(features_df)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:30:29.272325Z","iopub.execute_input":"2025-01-21T17:30:29.273960Z","iopub.status.idle":"2025-01-21T17:30:29.288849Z","shell.execute_reply":"2025-01-21T17:30:29.286940Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"# Model Training Functions","metadata":{}},{"cell_type":"code","source":"train_base_models <- function(features, labels) {\n  # Set up cross-validation\n  ctrl <- trainControl(method = \"cv\", \n                      number = 5,\n                      classProbs = TRUE)\n  \n  # Train base models with error handling\n  models <- list()\n  \n  tryCatch({\n    models$tree <- train(x = features, y = labels, \n                        method = \"rpart\", trControl = ctrl)\n  }, error = function(e) {\n    warning(\"Error in training tree model: \", e$message)\n  })\n  \n  tryCatch({\n    models$rf <- train(x = features, y = labels, \n                      method = \"rf\", \n                      trControl = ctrl,\n                      ntree = 50)  # Reduced for speed\n  }, error = function(e) {\n    warning(\"Error in training RF model: \", e$message)\n  })\n  \n  tryCatch({\n    models$svm <- train(x = features, y = labels, \n                       method = \"svmLinear\", \n                       trControl = ctrl)\n  }, error = function(e) {\n    warning(\"Error in training SVM model: \", e$message)\n  })\n  \n  return(models)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:30:23.131700Z","iopub.execute_input":"2025-01-21T17:30:23.133512Z","iopub.status.idle":"2025-01-21T17:30:23.147732Z","shell.execute_reply":"2025-01-21T17:30:23.145867Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"# Stacked Ensemble Function","metadata":{}},{"cell_type":"code","source":"train_stacked_ensemble <- function(base_predictions, labels) {\n  # Combine base model predictions\n  meta_features <- as.data.frame(base_predictions)\n  \n  # Train meta-learner (using elastic net)\n  meta_model <- train(x = meta_features, y = labels,\n                     method = \"glmnet\",\n                     trControl = trainControl(method = \"cv\", number = 5),\n                     tuneGrid = expand.grid(alpha = seq(0, 1, 0.1),\n                                          lambda = seq(0.0001, 1, length = 20)))\n  \n  return(meta_model)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:35.991038Z","iopub.execute_input":"2025-01-21T17:24:35.992735Z","iopub.status.idle":"2025-01-21T17:24:36.005719Z","shell.execute_reply":"2025-01-21T17:24:36.003994Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{}},{"cell_type":"code","source":"evaluate_predictions <- function(predictions, actual) {\n  # Convert factors to numeric if necessary\n  if(is.factor(predictions)) {\n    predictions <- as.numeric(as.character(predictions))\n  }\n  if(is.factor(actual)) {\n    actual <- as.numeric(as.character(actual))\n  }\n  \n  # Calculate metrics\n  rmse <- sqrt(mean((predictions - actual)^2))\n  \n  # Calculate F1 score (treating as multi-class classification)\n  f1 <- F1_Score(y_pred = predictions, \n                 y_true = actual, \n                 positive = unique(actual))\n  \n  return(list(\n    RMSE = rmse,\n    F1 = f1\n  ))\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:24:37.910543Z","iopub.execute_input":"2025-01-21T17:24:37.912127Z","iopub.status.idle":"2025-01-21T17:24:37.925812Z","shell.execute_reply":"2025-01-21T17:24:37.923997Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# options(warn=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:26:26.254745Z","iopub.execute_input":"2025-01-21T17:26:26.256429Z","iopub.status.idle":"2025-01-21T17:26:26.269226Z","shell.execute_reply":"2025-01-21T17:26:26.267319Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Main Function","metadata":{}},{"cell_type":"code","source":"asag_system <- function(train_data, test_data) {\n  # Map labels to valid R variable names\n  label_map <- create_label_mapping()\n  reverse_map <- reverse_label_mapping()\n  \n  # Convert labels in training and test data\n  train_data$label <- as.factor(label_map[as.character(train_data$label)])\n  test_data$label <- as.factor(label_map[as.character(test_data$label)])\n  \n  # Extract features\n  train_features <- extract_features(train_data)\n  test_features <- extract_features(test_data)\n  \n  # Ensure feature matrices have same columns\n  common_cols <- intersect(colnames(train_features), colnames(test_features))\n  train_features <- train_features[, common_cols]\n  test_features <- test_features[, common_cols]\n  \n  # Train base models\n  base_models <- train_base_models(train_features, train_data$label)\n  \n  # Get base model predictions\n  base_predictions <- lapply(base_models, predict, newdata = train_features)\n  base_predictions_test <- lapply(base_models, predict, newdata = test_features)\n  \n  # Train stacked ensemble\n  ensemble_model <- train_stacked_ensemble(do.call(cbind, base_predictions), \n                                         train_data$label)\n  \n  # Make final predictions\n  final_predictions <- predict(ensemble_model, \n                             newdata = as.data.frame(do.call(cbind, \n                                                           base_predictions_test)))\n  \n  # Convert predictions back to original numeric labels\n  final_predictions_numeric <- as.numeric(as.character(reverse_map[final_predictions]))\n  actual_numeric <- as.numeric(as.character(reverse_map[test_data$label]))\n  \n  # Evaluate predictions\n  evaluation <- evaluate_predictions(final_predictions_numeric, actual_numeric)\n  \n  return(list(\n    base_models = base_models,\n    ensemble_model = ensemble_model,\n    predictions = final_predictions_numeric,\n    evaluation = evaluation\n  ))\n}\n\n# Load and prepare data\ndata <- read.csv(\"/kaggle/input/asag-data/train.csv\")\n\n# Create train-test split (80-20)\nset.seed(42)\ntrain_index <- createDataPartition(data$label, p = 0.8, list = FALSE)\ntrain_data <- data[train_index, ]\ntest_data <- data[-train_index, ]\n\n# Run the ASAG system\nresult <- asag_system(train_data, test_data)\n\n# print(\"Evaluation Metrics:\")\n# print(paste(\"RMSE:\", result$evaluation$RMSE))\n# print(paste(\"F1 Score:\", result$evaluation$F1))\n\n# # Print confusion matrix\n# conf_matrix <- table(Predicted = result$predictions, \n#                     Actual = test_data$label)\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:30:46.717149Z","iopub.execute_input":"2025-01-21T17:30:46.718919Z","iopub.status.idle":"2025-01-21T17:31:57.556827Z","shell.execute_reply":"2025-01-21T17:31:57.555062Z"}},"outputs":[{"name":"stdout","text":"maximum number of iterations reached -8.436034e-05 -8.436536e-05maximum number of iterations reached 0.003168019 0.003080169maximum number of iterations reached 0.0004422935 0.0004422443maximum number of iterations reached -0.0001451932 0.0001452014maximum number of iterations reached 0.00098097 0.0009786619maximum number of iterations reached 0.001846696 0.00182968maximum number of iterations reached 0.0008518131 -0.000851444maximum number of iterations reached 0.004379824 0.004263404maximum number of iterations reached 0.0005451275 -0.0005448291maximum number of iterations reached 0.0005702644 0.0005701718maximum number of iterations reached 0.002266109 0.002255546maximum number of iterations reached 1.554261e-05 1.554358e-05maximum number of iterations reached 0.001285683 -0.00128454maximum number of iterations reached 0.001689908 0.001684898maximum number of iterations reached 0.0003761772 0.0003761393maximum number of iterations reached 0.0009771355 -0.0009768019maximum number of iterations reached 0.001528081 0.001520886maximum number of iterations reached 0.002253433 0.002225167maximum number of iterations reached 0.000264463 -0.0002644396[1] \"Evaluation Metrics:\"\n[1] \"RMSE: 1.75086384209292\"\n[1] \"F1 Score: 0.772667542706964\"\n[1] \"Confusion Matrix:\"\n         Actual\nPredicted   0   1   2   3   4\n        1 294  46 158  83   1\n        2  49  23  41  99   2\n        4  59  30  65  42   0\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Print evaluation metrics\n\nprint(\"Evaluation Metrics:\")\nprint(paste(\"RMSE:\", result$evaluation$RMSE))\nprint(paste(\"F1 Score:\", result$evaluation$F1))\n\n# Print confusion matrix\nconf_matrix <- table(Predicted = result$predictions, \n                    Actual = test_data$label)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:33:30.632710Z","iopub.execute_input":"2025-01-21T17:33:30.634490Z","iopub.status.idle":"2025-01-21T17:33:30.666774Z","shell.execute_reply":"2025-01-21T17:33:30.665070Z"}},"outputs":[{"name":"stdout","text":"[1] \"Evaluation Metrics:\"\n[1] \"RMSE: 1.75086384209292\"\n[1] \"F1 Score: 0.772667542706964\"\n[1] \"Confusion Matrix:\"\n         Actual\nPredicted   0   1   2   3   4\n        1 294  46 158  83   1\n        2  49  23  41  99   2\n        4  59  30  65  42   0\n","output_type":"stream"}],"execution_count":52}]}